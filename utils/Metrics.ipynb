{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1263d47",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8202af51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import nltk\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import collections as coll\n",
    "from tqdm import tqdm\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "566e3c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('punkt')\n",
    "st = [\",\", \".\", \"'\", \"!\", '\"', \"#\", \"$\", \"%\", \"&\", \"(\", \")\", \"*\", \"+\", \"-\", \".\", \"/\", \":\", \";\", \"<\", \"=\", '>', \"?\",\n",
    "      \"@\", \"[\", \"\\\\\", \"]\", \"^\", \"_\", '`', \"{\", \"|\", \"}\", '~', '\\t', '\\n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f25e203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46b83249",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:05<00:00,  1.03it/s]\n"
     ]
    }
   ],
   "source": [
    "main_characters = ['ДЖОУИ', 'МОНИКА', 'РЕЙЧЕЛ', 'РОСС', 'ФИБИ', 'ЧЕНДЛЕР']\n",
    "main_characters = sorted(main_characters)\n",
    "texts_ru = {i : \"\" for i in main_characters}\n",
    "texts_eng = {i : \"\" for i in main_characters}\n",
    "\n",
    "for name in tqdm(main_characters):\n",
    "    url_ru = f'https://raw.githubusercontent.com/Alenush/style_transfer_sirius2021summer/master/data/scripts/russian/{name}.txt'\n",
    "    req = requests.get(url_ru)\n",
    "    if req.status_code == requests.codes.ok:\n",
    "        req = BeautifulSoup(req.text, 'html.parser')\n",
    "        texts_ru[name] = str(req)\n",
    "    else:\n",
    "        print('Content was not found.')\n",
    "        \n",
    "    url_eng = f'https://raw.githubusercontent.com/Alenush/style_transfer_sirius2021summer/master/data/scripts/english/{name}.txt'\n",
    "    req = requests.get(url_eng)      \n",
    "    if req.status_code == requests.codes.ok:\n",
    "        req = BeautifulSoup(req.text, 'html.parser')\n",
    "        texts_eng[name] = str(req)\n",
    "    else:\n",
    "        print('Content was not found.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea4ecbf",
   "metadata": {},
   "source": [
    "## Promptly for paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c94f8cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:17,  2.86s/it]\n"
     ]
    }
   ],
   "source": [
    "# amount of english words per character\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "amount_en = dict()\n",
    "amount_ru = dict()\n",
    "for name, name_en in tqdm(zip(main_characters, ['Joey', 'Monica', 'Rachel', 'Ross', 'Phoebe', 'Chandler'])):\n",
    "    text = texts_eng[name]\n",
    "    amount_en[name_en] = [len(sent_tokenize(text)), len(word_tokenize(text))]\n",
    "    text = texts_ru[name]\n",
    "    amount_ru[name_en] = [len(sent_tokenize(text)), len(word_tokenize(text))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c9ef3a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrr}\n",
      "\\toprule\n",
      "character &  eng\\_sentences &  eng\\_words &  ru\\_sentences &  ru\\_words \\\\\n",
      "\\midrule\n",
      "     Joey &          14590 &     114341 &         14562 &    100582 \\\\\n",
      "   Monica &          13228 &     105350 &         13182 &     90818 \\\\\n",
      "   Rachel &          15664 &     126285 &         15593 &    110510 \\\\\n",
      "     Ross &          15580 &     126339 &         15524 &    109708 \\\\\n",
      "   Phoebe &          13053 &     104681 &         12965 &     91313 \\\\\n",
      " Chandler &          13060 &     109603 &         13039 &     95744 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_stats = pd.DataFrame([(i, j[0] , j[1]) for i, j in amount_en.items()], \n",
    "                        columns=['character','eng_sentences','eng_words'])\n",
    "df_stats['ru_sentences'] = [(j[0]) for i, j in amount_ru.items()]\n",
    "df_stats['ru_words'] = [(j[1]) for i, j in amount_ru.items()]\n",
    "df_stats = df_stats[['character','eng_sentences','eng_words','ru_sentences','ru_words']]\n",
    "print(df_stats.to_latex(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a9797d",
   "metadata": {},
   "source": [
    "## Lexical features \n",
    "#### Russian\n",
    "#### https://github.com/ivbeg/readability.io/wiki/API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a0cc6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:08<00:00,  1.47s/it]\n"
     ]
    }
   ],
   "source": [
    "texts = texts_ru\n",
    "\n",
    "metrics = []\n",
    "for name in tqdm(main_characters):\n",
    "    url_ru = f'https://raw.githubusercontent.com/Alenush/style_transfer_sirius2021summer/master/data/scripts/russian/{name}.txt'\n",
    "    response = requests.post(f\"http://api.plainrussian.ru/api/1.0/ru/measure/?url={url_ru}\")\n",
    "    metrics.append((name, response.json()))\n",
    "metrics = sorted(metrics, key=lambda x: x[1]['indexes']['index_SMOG'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b28dae7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dct = dict(metrics)\n",
    "complex_words = np.array([(name, int(metrics_dct[name]['metrics']['n_complex_words'])) for name in main_characters])\n",
    "df_metrics = pd.DataFrame(complex_words)\n",
    "df_metrics.columns = ['name', 'complex_words']\n",
    "\n",
    "df_metrics.name = df_metrics.name.str.replace(\"ДЖОУИ\", \"Joey\")\n",
    "df_metrics.name = df_metrics.name.str.replace(\"МОНИКА\", \"Monica\")\n",
    "df_metrics.name = df_metrics.name.str.replace(\"РЕЙЧЕЛ\", \"Rachel\")\n",
    "df_metrics.name = df_metrics.name.str.replace(\"РОСС\", \"Ross\")\n",
    "df_metrics.name = df_metrics.name.str.replace(\"ФИБИ\", \"Phoebe\")\n",
    "df_metrics.name = df_metrics.name.str.replace(\"ЧЕНДЛЕР\", \"Chandler\")\n",
    "\n",
    "df_metrics.complex_words = df_metrics.complex_words.astype(int)\n",
    "avg_slen  = np.array([(name, int(metrics_dct[name]['metrics']['avg_slen'])) for name in main_characters]).T[1]\n",
    "df_metrics['avg_slen'] = avg_slen.astype(int)\n",
    "n_words  = np.array([(name, int(metrics_dct[name]['metrics']['n_words'])) for name in main_characters]).T[1]\n",
    "df_metrics['n_words'] = n_words.astype(int)\n",
    "n_sentences  = np.array([(name, int(metrics_dct[name]['metrics']['n_sentences'])) for name in main_characters]).T[1]\n",
    "df_metrics['n_sentences'] = n_sentences.astype(int) \n",
    "c_share  = np.array([(name, metrics_dct[name]['metrics']['c_share']) for name in main_characters]).T[1]\n",
    "df_metrics['c_share'] = c_share.astype(float)\n",
    "\n",
    "\n",
    "# fig, axs = plt.subplots(2, 3, figsize=(20,10))\n",
    "# axs = axs.flatten()\n",
    "\n",
    "# ax = axs[0]\n",
    "# ax = sns.barplot(x=\"name\", y=\"n_words\", data=df_metrics, ax=ax)\n",
    "# ax.set(xlabel='Персонаж', ylabel='Кол-во слов', title='Общее количество слов')\n",
    "\n",
    "# ax = axs[1]\n",
    "# ax = sns.barplot(x=\"name\", y=\"complex_words\", data=df_metrics, ax=ax)\n",
    "# ax.set(xlabel='Персонаж', ylabel='Кол-во слов', title='Количество сложных слов')\n",
    "\n",
    "# ax = axs[2]\n",
    "# ax = sns.barplot(x=\"name\", y=\"avg_slen\", data=df_metrics, ax=ax)\n",
    "# ax.set(xlabel='Персонаж', ylabel='Кол-во слов', title='Среднее число слов в предложении')\n",
    "\n",
    "# ax = axs[3]\n",
    "# ax = sns.barplot(x=\"name\", y=\"n_sentences\", data=df_metrics, ax=ax)\n",
    "# ax.set(xlabel='Персонаж', ylabel='Кол-во предложений', title='Общее количество предложений')\n",
    " \n",
    "# ax = axs[4]\n",
    "# ax = sns.barplot(x=\"name\", y=\"c_share\", data=df_metrics, ax=ax)\n",
    "# ax.set(xlabel='Персонаж', ylabel='Доля (%)', title='Доля сложных слов от общего числа')\n",
    " \n",
    "# fig.patch.set_facecolor('white')\n",
    "    \n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b4d1723",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_width(ax, new_value) :\n",
    "    for patch in ax.patches :\n",
    "        current_width = patch.get_width()\n",
    "        diff = current_width - new_value\n",
    "        # we change the bar width\n",
    "        patch.set_width(new_value)\n",
    "        # we recenter the bar\n",
    "        patch.set_x(patch.get_x() + diff * .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8949b73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:01,  3.49it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14195.833333333334"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# amount of english words per character\n",
    "from nltk.tokenize import sent_tokenize\n",
    "amount_sent = dict()\n",
    "for name, name_en in tqdm(zip(main_characters, ['Joey', 'Monica', 'Rachel', 'Ross', 'Phoebe', 'Chandler'])):\n",
    "    text = texts_eng[name]\n",
    "    amount_sent[name_en] = len(sent_tokenize(text))\n",
    "np.array(list(amount_sent.values())).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a4f97b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics['n_sentences_en'] = df_metrics.name.apply(lambda x: amount_sent[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec17b28",
   "metadata": {},
   "source": [
    "## Vocabulary metrics\n",
    "#### https://github.com/Hassaan-Elahi/Writing-Styles-Classification-Using-Stylometric-Analysis/blob/master/Code/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15527fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hapaxLegemena(words):\n",
    "    ### only one instance of use\n",
    "    \n",
    "    V1 = 0\n",
    "    # dictionary comprehension . har word kay against value 0 kardi\n",
    "    freqs = {key: 0 for key in words}\n",
    "    for word in words:\n",
    "        freqs[word] += 1\n",
    "    for word in freqs:\n",
    "        if freqs[word] == 1:\n",
    "            V1 += 1\n",
    "    N = len(words)\n",
    "    V = float(len(set(words)))\n",
    "    R = 100 * math.log(N) / max(1, (1 - (V1 / V)))\n",
    "    h = V1 / N\n",
    "    return h\n",
    "\n",
    "def ShannonEntropy(words):\n",
    "    # -1*sigma(pi*lnpi)\n",
    "    # Shannon and sympsons index are basically diversity indices for any community\n",
    "    \n",
    "    lenght = len(words)\n",
    "    freqs = coll.Counter()\n",
    "    freqs.update(words)\n",
    "    arr = np.array(list(freqs.values()))\n",
    "    distribution = 1. * arr\n",
    "    distribution /= max(1, lenght)\n",
    "    import scipy as sc\n",
    "    H = sc.stats.entropy(distribution, base=2)\n",
    "    # H = sum([(i/lenght)*math.log(i/lenght,math.e) for i in freqs.values()])\n",
    "    return H\n",
    "\n",
    "def YulesCharacteristicK(words):\n",
    "    # K  10,000 * (M - N) / N**2\n",
    "    # , where M  Sigma i**2 * Vi.\n",
    "    N = len(words)\n",
    "    freqs = coll.Counter()\n",
    "    freqs.update(words)\n",
    "    vi = coll.Counter()\n",
    "    vi.update(freqs.values())\n",
    "    M = sum([(value * value) * vi[value] for key, value in freqs.items()])\n",
    "    K = 10000 * (M - N) / math.pow(N, 2)\n",
    "    return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b7d78ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-730e812bb635>:42: RuntimeWarning: overflow encountered in long_scalars\n",
      "  K = 10000 * (M - N) / math.pow(N, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7.07 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>hapaxLegemena</th>\n",
       "      <th>ShannonEntropy</th>\n",
       "      <th>Yules</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ДЖОУИ</td>\n",
       "      <td>0.093613</td>\n",
       "      <td>10.273897</td>\n",
       "      <td>0.243041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>МОНИКА</td>\n",
       "      <td>0.099345</td>\n",
       "      <td>10.253189</td>\n",
       "      <td>0.305831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>РЕЙЧЕЛ</td>\n",
       "      <td>0.081088</td>\n",
       "      <td>9.990844</td>\n",
       "      <td>0.168498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>РОСС</td>\n",
       "      <td>0.093483</td>\n",
       "      <td>10.271940</td>\n",
       "      <td>0.256537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ФИБИ</td>\n",
       "      <td>0.104359</td>\n",
       "      <td>10.251260</td>\n",
       "      <td>0.295524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ЧЕНДЛЕР</td>\n",
       "      <td>0.103805</td>\n",
       "      <td>10.333549</td>\n",
       "      <td>-0.349289</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      name  hapaxLegemena  ShannonEntropy     Yules\n",
       "0    ДЖОУИ       0.093613       10.273897  0.243041\n",
       "1   МОНИКА       0.099345       10.253189  0.305831\n",
       "2   РЕЙЧЕЛ       0.081088        9.990844  0.168498\n",
       "3     РОСС       0.093483       10.271940  0.256537\n",
       "4     ФИБИ       0.104359       10.251260  0.295524\n",
       "5  ЧЕНДЛЕР       0.103805       10.333549 -0.349289"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "voc_metrics = []\n",
    "for name in main_characters:\n",
    "    text = nltk.tokenize.word_tokenize(texts[name])\n",
    "    words = [word for word in text if word not in st]\n",
    "    \n",
    "    voc_metrics_name = [name]\n",
    "    voc_metrics_name.append(hapaxLegemena(words))\n",
    "    voc_metrics_name.append(ShannonEntropy(words))\n",
    "    voc_metrics_name.append(YulesCharacteristicK(words))\n",
    "    voc_metrics.append(voc_metrics_name)\n",
    "df_voc_metrics = pd.DataFrame(voc_metrics)\n",
    "df_voc_metrics.columns = ['name', 'hapaxLegemena', 'ShannonEntropy', 'Yules']\n",
    "df_voc_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe2855c",
   "metadata": {},
   "source": [
    "## Readability metrics\n",
    "#### https://github.com/Hassaan-Elahi/Writing-Styles-Classification-Using-Stylometric-Analysis/blob/master/Code/main.py\n",
    "\n",
    "##### Upd: расчет Readability index по ссылке не подходит для русского "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa23211d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cmuDictionary = None\n",
    "\n",
    "# def syllable_count_Manual(word):\n",
    "#     word = word.lower()\n",
    "#     count = 0\n",
    "#     vowels = \"ауоыиэяюёе\"\n",
    "#     if word[0] in vowels:\n",
    "#         count += 1\n",
    "#     for index in range(1, len(word)):\n",
    "#         if word[index] in vowels and word[index - 1] not in vowels:\n",
    "#             count += 1\n",
    "#             if word.endswith(\"e\"):\n",
    "#                 count -= 1\n",
    "#     if count == 0:\n",
    "#         count += 1\n",
    "#     return count\n",
    "\n",
    "# def syllable_count(word):\n",
    "#     global cmuDictionary\n",
    "#     d = cmuDictionary\n",
    "#     try:\n",
    "#         syl = [len(list(y for y in x if y[-1].isdigit())) for x in d[word.lower()]][0]\n",
    "#     except:\n",
    "#         syl = syllable_count_Manual(word)\n",
    "#     return syl\n",
    "\n",
    "# def FleschReadingEase(words, NoOfsentences):\n",
    "#     l = float(len(words))\n",
    "#     scount = 0\n",
    "#     for word in words:\n",
    "#         scount += syllable_count(word)\n",
    "\n",
    "#     I = 206.835 - 1.015 * (l / float(NoOfsentences)) - 84.6 * (scount / float(l))\n",
    "#     return I\n",
    "\n",
    "# # def dale_chall_readability_formula(words, NoOfSectences):\n",
    "# #     difficult = 0\n",
    "# #     adjusted = 0\n",
    "# #     NoOfWords = len(words)\n",
    "# #     with open('dale-chall.pkl', 'rb') as f:\n",
    "# #         fimiliarWords = pickle.load(f)\n",
    "# #     for word in words:\n",
    "# #         if word not in fimiliarWords:\n",
    "# #             difficult += 1\n",
    "# #     percent = (difficult / NoOfWords) * 100\n",
    "# #     if (percent > 5):\n",
    "# #         adjusted = 3.6365\n",
    "# #     D = 0.1579 * (percent) + 0.0496 * (NoOfWords / NoOfSectences) + adjusted\n",
    "# #     return D\n",
    "\n",
    "# def GunningFoxIndex(words, NoOfSentences):\n",
    "#     NoOFWords = float(len(words))\n",
    "#     complexWords = 0\n",
    "#     for word in words:\n",
    "#         if (syllable_count(word) > 2):\n",
    "#             complexWords += 1\n",
    "\n",
    "#     G = 0.4 * ((NoOFWords / NoOfSentences) + 100 * (complexWords / NoOFWords))\n",
    "#     return G\n",
    "\n",
    "# # winSize = 10\n",
    "# # voc_metrics = []\n",
    "# # for name in main_characters:\n",
    "# #     text = nltk.tokenize.word_tokenize(texts[name])\n",
    "# #     words = [word for word in text if word not in st]\n",
    "    \n",
    "# #     voc_metrics_name = [name]\n",
    "# #     voc_metrics_name.append(FleschReadingEase(words, winSize))\n",
    "# # #     voc_metrics_name.append(dale_chall_readability_formula(words, winSize))\n",
    "# #     voc_metrics_name.append(GunningFoxIndex(words, winSize))\n",
    "# #     voc_metrics.append(voc_metrics_name)\n",
    "# # df_voc_metrics = pd.DataFrame(voc_metrics)\n",
    "# # df_voc_metrics.columns = ['name', 'Flesch', 'GunFox']\n",
    "# # df_voc_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e1a13d",
   "metadata": {},
   "source": [
    "## Sentiment analysis\n",
    "#### https://github.com/cyberdh/Text-Analysis/tree/master/VADERSentimentAnalysis\n",
    "##### Смотрим соотношение положительной/ негативной лексики у персонажей.\n",
    "##### NB: используем скрипты на английском"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9546af4",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'style_transfer_sirius2021summer'...\n",
      "Updating files:  79% (529/664)\n",
      "Updating files:  80% (532/664)\n",
      "Updating files:  81% (538/664)\n",
      "Updating files:  82% (545/664)\n",
      "Updating files:  83% (552/664)\n",
      "Updating files:  84% (558/664)\n",
      "Updating files:  85% (565/664)\n",
      "Updating files:  86% (572/664)\n",
      "Updating files:  87% (578/664)\n",
      "Updating files:  88% (585/664)\n",
      "Updating files:  89% (591/664)\n",
      "Updating files:  90% (598/664)\n",
      "Updating files:  91% (605/664)\n",
      "Updating files:  92% (611/664)\n",
      "Updating files:  93% (618/664)\n",
      "Updating files:  94% (625/664)\n",
      "Updating files:  95% (631/664)\n",
      "Updating files:  96% (638/664)\n",
      "Updating files:  97% (645/664)\n",
      "Updating files:  98% (651/664)\n",
      "Updating files:  99% (658/664)\n",
      "Updating files: 100% (664/664)\n",
      "Updating files: 100% (664/664), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Alenush/style_transfer_sirius2021summer.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b37a3798",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd style_transfer_sirius2021summer\n",
    "!git checkout master\n",
    "!git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f72302b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd\n",
    "!cd data/data_for_tone_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "380ec31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "posWords = pd.read_csv(\"posWords.csv\", index_col=False).iloc[:, 1:]\n",
    "pos_list = posWords.values.T[0] \n",
    "negWords = pd.read_csv(\"negWords.csv\", index_col=False).iloc[:, 1:]\n",
    "neg_list = negWords.values.T[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd5fc97a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:33<00:00, 15.36s/it]\n"
     ]
    }
   ],
   "source": [
    "en_text = {name : [] for name in main_characters}\n",
    "en_text['НЕДРУГ'] = []\n",
    "\n",
    "for i in tqdm(range(1, 11)):\n",
    "    url = f'https://github.com/Alenush/style_transfer_sirius2021summer/tree/master/data/scripts/english/{i}'\n",
    "    req = requests.get(url)\n",
    "    soup = BeautifulSoup(req.content, 'html.parser')\n",
    "    for j in soup.find_all('span', {\"class\": \"css-truncate css-truncate-target d-block width-fit\"}):\n",
    "        url = f'https://raw.githubusercontent.com' + j.find('a')['href'].replace('blob/', '') \n",
    "        req = requests.get(url)\n",
    "        if req.status_code == requests.codes.ok:\n",
    "            soup = str(BeautifulSoup(req.text, 'html.parser'))\n",
    "        else:\n",
    "            print('Content was not found.')\n",
    "        script = []\n",
    "        for line in soup.split('\\n'):\n",
    "            if len(line) != 0 and line[0].isupper():\n",
    "                tag = line.find(\":\")\n",
    "                script.append((line[:tag], line[tag + 1:].strip()))\n",
    "        for person in set([i[0] for i in script]):\n",
    "            words = [i[1] for i in script if i[0] == person]\n",
    "            en_text[person] += words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "759588a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ch = sorted(list(set([i[0] for i in script])))\n",
    "en_bags = en_text.copy()\n",
    "for person in ch:\n",
    "    # person = ch[0]\n",
    "    seq = \"\\n\".join(en_text[person])\n",
    "    seq = re.sub(\"<(.*?)>\", \"\", seq)\n",
    "    for i in st:\n",
    "        if i == \"'\":\n",
    "            continue\n",
    "        if i in ('(', ')', '*', '+', '?', '[', '\\\\', '^'):\n",
    "            seq = re.sub('[\\\\' + f'{i}]', \" \", seq)\n",
    "        else:\n",
    "            seq = re.sub(f'[{i}]', \" \", seq)\n",
    "        seq = re.sub(\" +\", \" \", seq)\n",
    "    seq = seq.lower()\n",
    "    seq = seq.split()\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    seq_lem = [lemmatizer.lemmatize(s, pos='v') for s in seq]\n",
    "    en_bags[person] = seq_lem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1f384cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_res = dict()\n",
    "for person in ch:\n",
    "    person_bag = en_bags[person]\n",
    "    person_pos = {w : person_bag.count(w) for w in pos_list}\n",
    "    person_neg = {w : person_bag.count(w) for w in neg_list}\n",
    "    sentiment_res[person] = (person_pos, person_neg)\n",
    "    \n",
    "pos = []\n",
    "neg = []\n",
    "for name in sentiment_res:\n",
    "    pos.append(sum(list(sentiment_res[name][0].values())))\n",
    "    neg.append(sum(list(sentiment_res[name][1].values())))\n",
    "posneg = np.array([pos, neg])\n",
    "posneg_sh = posneg[0] / (posneg[0] + posneg[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877b6831",
   "metadata": {},
   "outputs": [],
   "source": [
    "persons = list(sentiment_res.keys())\n",
    "index = np.arange(len(persons))\n",
    "bar_width = 0.35\n",
    "matplotlib.rcParams.update({'font.size': 10})\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15,5))\n",
    "axs = axs.flatten()\n",
    "\n",
    "ax = axs[0]\n",
    "pos_plot = ax.bar(index, pos, bar_width, label='полож.', color='b')\n",
    "neg_plot = ax.bar(index+bar_width, neg, bar_width, label='нег.', color='r')\n",
    "ax.set_xlabel('Персонаж')\n",
    "ax.set_ylabel('Кол-во слов')\n",
    "ax.set_title('Количество полож./ нег. слов')\n",
    "ax.set_xticks(index + bar_width / 2)\n",
    "ax.set_xticklabels(persons)\n",
    "ax.legend()\n",
    "\n",
    "ax = axs[1]\n",
    "# ax.bar(persons, (posneg_sh*0+1), label='полож.', color='r')\n",
    "ax.bar(persons, posneg_sh, label='полож.', color='b')\n",
    "ax.set_xlabel('Персонаж')\n",
    "ax.set_ylabel('Доля')\n",
    "ax.set_title('Доля положительных слов')\n",
    "ax.set(ylim=(0.7, 0.75))\n",
    "\n",
    "fig.patch.set_facecolor('white')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d38317",
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams.update({'font.size': 16})\n",
    "persons = list(sentiment_res.keys())\n",
    "index = np.arange(len(persons))\n",
    "fig, axs = plt.subplots(1, 1, figsize=(15,10), facecolor='w')\n",
    "bar_width = 0.35\n",
    "ax = axs\n",
    "\n",
    "persons_en = ['Joey', 'Monica', 'Other', 'Rachel', 'Ross', 'Phoebe', 'Chandler']\n",
    "\n",
    "df_tmp = pd.DataFrame([persons_en, pos, neg]).T\n",
    "df_tmp.columns = ['char', 'pos', 'neg']\n",
    "df_tmp = df_tmp.sort_values(by=['pos'])\n",
    "pos_plot = ax.bar(index-bar_width/2, df_tmp.pos, bar_width, label='Positive', color='b')\n",
    "neg_plot = ax.bar(index+bar_width/2, df_tmp.neg, bar_width, label='Negative', color='r')\n",
    "ax.set_xlabel('Character')\n",
    "ax.set_ylabel('Words')\n",
    "ax.set_title('Amount of positive and negative words')\n",
    "ax.set_xticks(index)\n",
    "ax.set_xticklabels(df_tmp.char)\n",
    "ax.legend()\n",
    "\n",
    "# for index, row in df_metrics.iterrows():\n",
    "#     ax.text(row.name, row.n_words, row.n_words, color='black', ha=\"center\", va=\"bottom\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
